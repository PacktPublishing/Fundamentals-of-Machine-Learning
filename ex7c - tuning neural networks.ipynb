{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ex7c - tuning neural networks.ipynb","provenance":[{"file_id":"1uM7CzWPLifgf16aVtJhWeOGN6A0guvlv","timestamp":1645424645173},{"file_id":"1cj_EGkLHy8p-Q4qJsfPbbVbJ55_n_A0k","timestamp":1638891071765}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pzl2fK3J1QoT"},"source":["### Building Models Using Sequential API\n","\n","We have covered Sequential API and we have demonstrated standard pipeline to build, train, evaluate a model that carries 10-class classification task. \n","\n","This week we will come back to regression problem. The main differences are the fact that the output layer has a single neuron (since we only want to predict a single value) and uses no activation function in the end. In this case, in training the loss function is simply mean squared error. \n","\n","Let us build this using Sequential API first. "]},{"cell_type":"code","metadata":{"id":"KAc-jAhfiCN8"},"source":["# library\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iFvH8LK-ifeH"},"source":["# data\n","housing = fetch_california_housing()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CA8V4B2CiyQz"},"source":["# train, validate, and test\n","X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n","X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRT6vt2UjIIa"},"source":["# standardize\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train) # What is training set?\n","X_valid = scaler.transform(X_valid) # What is validating set?\n","X_test = scaler.transform(X_test) # What is test set?"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Sequential API\n","\n","Let us use Sequential API to build a neural network model."],"metadata":{"id":"CGeWURFCg-86"}},{"cell_type":"code","metadata":{"id":"gdR_lSiQjWqp"},"source":["# sequential API\n","\n","# How many neurons does this network has?\n","# How many hidden layers does this network have?\n","\n","model = tf.keras.models.Sequential([\n","     tf.keras.layers.Dense(30, activation=tf.nn.relu), \n","     tf.keras.layers.Dense(1)\n","  ]) # softmax is designed for multi-class classification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOURLCvdj1dv"},"source":["# compile\n","model.compile(\n","    loss='mean_squared_error', # Why do we use mean squared error?\n","    optimizer='sgd' # What is the difference between \n","    # stochastic gradient descent (SGD) and gradient descent (GD)?\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3E9zlAsVkBaz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639328458410,"user_tz":480,"elapsed":21171,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"01e8bf74-ca76-4a14-9f38-c9d8d857c246"},"source":["# fit the model\n","history = model.fit(\n","    X_train, y_train, epochs=20,\n","    validation_data=(X_valid, y_valid)\n","    # Notice that we fill in X_valid and y_valid, why don't we put in \n","    # X_test, y_test?\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","363/363 [==============================] - 1s 2ms/step - loss: 1.2573 - val_loss: 0.5914\n","Epoch 2/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.5355 - val_loss: 0.4928\n","Epoch 3/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.5076 - val_loss: 0.4488\n","Epoch 4/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.4564 - val_loss: 0.4276\n","Epoch 5/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.4240 - val_loss: 0.4195\n","Epoch 6/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.4087 - val_loss: 0.4108\n","Epoch 7/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.3998 - val_loss: 0.3996\n","Epoch 8/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.3949 - val_loss: 0.3978\n","Epoch 9/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.3892 - val_loss: 0.3982\n","Epoch 10/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.3853 - val_loss: 0.3865\n","Epoch 11/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.3959 - val_loss: 0.3983\n","Epoch 12/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.3840 - val_loss: 0.3979\n","Epoch 13/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.3790 - val_loss: 0.3756\n","Epoch 14/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.3723 - val_loss: 0.3750\n","Epoch 15/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.3705 - val_loss: 0.3678\n","Epoch 16/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.3656 - val_loss: 0.3647\n","Epoch 17/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.3657 - val_loss: 0.3664\n","Epoch 18/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.3616 - val_loss: 0.3623\n","Epoch 19/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.3682 - val_loss: 0.3647\n","Epoch 20/20\n","363/363 [==============================] - 1s 2ms/step - loss: 0.3631 - val_loss: 0.3629\n"]}]},{"cell_type":"code","metadata":{"id":"mC-es_aEkJvV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639328458709,"user_tz":480,"elapsed":335,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"55839bea-ce63-4e10-d2ab-21440180d95e"},"source":["# compute test set error\n","mse_test = model.evaluate(X_test, y_test)\n","mse_test"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["162/162 [==============================] - 0s 1ms/step - loss: 0.3722\n"]},{"output_type":"execute_result","data":{"text/plain":["0.3721841871738434"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"oxXbf1WZkV1F"},"source":["# get an arbitrary new observation\n","X_new = X_test[:3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIe_E43Lkdlb"},"source":["# predict using arbitrar X defined above\n","y_pred = model.predict(X_new)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JAmAcca7kiLX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639328459086,"user_tz":480,"elapsed":385,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"1a6c217e-d1d5-4b50-d355-7f43ddb6e7aa"},"source":["# check outcome\n","y_pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2.0838928],\n","       [3.287313 ],\n","       [1.4829155]], dtype=float32)"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"mLuVG_UhkupV"},"source":["The Sequential API is quite easy to use. However, althrough Sequential models are extremely common, it is sometimes useful to build neural networks with more complex topologies, or with multiple inputs or outputs. For this purpose, we introduce Functional API. "]},{"cell_type":"markdown","source":["#### Performance"],"metadata":{"id":"uxIh4tJShWyk"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# plot training and validating performance\n","pd.DataFrame(history.history).plot(figsize=(8,5))\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"8gTql92ngtbG","colab":{"base_uri":"https://localhost:8080/","height":320},"executionInfo":{"status":"ok","timestamp":1639328459089,"user_tz":480,"elapsed":24,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"f10028f9-eb61-4b0c-bd1e-149017e77fc6"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x360 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["### Functions in Python\n","\n","A function is a block of code which only runs when it is called. You can pass data, known as parameters, into a function. A function can return data as a result.\n","\n","```\n","def YOUR_FUNCTION(INPUT):\n","    return OUTPUT\n","```"],"metadata":{"id":"By-dPrF6-77i"}},{"cell_type":"code","source":["# build a basic function\n","def my_function():\n","  print(\"Hello from a function\")"],"metadata":{"id":"g1a_8o4yNOQa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# call a function that is pre-defined\n","my_function()"],"metadata":{"id":"95b6G_0gNUSS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From above, we have seen a basic form of a python function. However, often times in practice we need to have the function execute some command based on an input. \n","\n","This input is called an argument. Let us see the following example."],"metadata":{"id":"gBGFPCOCNbKN"}},{"cell_type":"code","source":["# define\n","def my_function(fname):\n","  print(\"Hi, \" + fname + \"! Good morning!\")"],"metadata":{"id":"iJP8-ZF9Nnnu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# try\n","my_function(\"Patrick\")"],"metadata":{"id":"AMV7-BUpNtku","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639328459568,"user_tz":480,"elapsed":30,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"e358bd06-eba2-4ef4-995b-b134300d97fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hi, Patrick! Good morning!\n"]}]},{"cell_type":"markdown","source":["A function can take more than one arguments."],"metadata":{"id":"67BOVnixOBC1"}},{"cell_type":"code","source":["# define\n","def my_function(first_name, last_name):\n","    print(\"Morning! \" + first_name + \" \" + last_name)"],"metadata":{"id":"uBImo15fOE8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run\n","my_function(\"James\", \"Bond\")"],"metadata":{"id":"uHamk_pDOQu-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639328459569,"user_tz":480,"elapsed":28,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"2ce031d3-b34e-4b84-ecb8-1ff820a62f64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Morning! James Bond\n"]}]},{"cell_type":"markdown","source":["In machine learning, sometimes we default a parameter value in order to have the code produce some preliminary results. Let us introduce this default function."],"metadata":{"id":"HeWSZrErOhEE"}},{"cell_type":"code","source":["# define\n","def my_function(a = 3):\n","    print(\"The number you entered is \" + str(a) + \" and the next integer is \" + str(a+1))"],"metadata":{"id":"ACJfM_QOOqlU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run\n","my_function() # this produce default input"],"metadata":{"id":"ZM4r3z3EO0kV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639328463295,"user_tz":480,"elapsed":20,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"d6738599-bd2f-48ba-f936-775aeb2b9afd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number you entered is 3 and the next integer is 4\n"]}]},{"cell_type":"code","source":["# run\n","my_function(2) # the input is now defined as \"2\""],"metadata":{"id":"snRuU_YGO7ax","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639328464625,"user_tz":480,"elapsed":20,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"82e79d29-de03-466f-e679-12a0dbd68313"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The number you entered is 2 and the next integer is 3\n"]}]},{"cell_type":"markdown","source":["A function can return an object as you desire. To do this, we can use the *return* syntax."],"metadata":{"id":"FSOvVG-QPTsX"}},{"cell_type":"code","source":["# define\n","def my_function(a=3, b=2):\n","    return a+b**2"],"metadata":{"id":"-WV9SJlUPZL8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run\n","my_function() # What do you think would be the default result?"],"metadata":{"id":"w4YcvEvjPlCs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639328467837,"user_tz":480,"elapsed":12,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"79b6a5e5-5c70-4761-ecd3-4961fe08ef79"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["# run\n","my_function(4,2) # What do you think the result is?"],"metadata":{"id":"uBYQDjkdPfJr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639328469148,"user_tz":480,"elapsed":24,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"8b696a0e-1889-4d2f-be55-ca62343b72ee"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"M35LOVYx2wYW"},"source":["### Fine-Tuning Neural Network Hyperparameters\n","\n","The flexibility of neural networks is also one of the main drawbacks: there are many hyperparameters to tweak. Not only can you use any imaginable network architecture, but even in a simple neural network you can change the number of layers drastically, the number of neurons per layer, the type of activation function to use in each layer, the weight initialization logic, and much more. How do we know what combination of hyperparameters is the best for your task?\n","\n","One option is to try many different combinations of hyperparameters and see which one works best on the validation set (or use K-fold cross validation). "]},{"cell_type":"code","metadata":{"id":"g1ZD4k1osM-A"},"source":["# define model\n","def build_model(n_hidden=1, n_neurons=30, learning_rate=0.001, input_shape=[8]):\n","    model = tf.keras.models.Sequential()\n","\n","    # What type of API are we using for input layer?\n","    model.add(tf.keras.layers.InputLayer(input_shape=input_shape))\n","\n","    # What type of API are we using for hidden layer?\n","    for layer in range(n_hidden):\n","        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n","\n","    # Why do we set number of neurons (or units) to be 1 for this following layer?\n","    model.add(tf.keras.layers.Dense(1))\n","\n","    # A gentle reminder question: What is the difference between \n","    # stochastic gradient descent and gradient descent?\n","    optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n","\n","    # Another gentle reminder question: Why do we use mse or mean squared error？\n","    model.compile(loss=\"mse\", optimizer=optimizer)\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Recall in Week 2 we introduced linear regression. In the code walk through, we used Sci-kit Learn library to import *LinearRegression()* function and we were able to fit the data using the *.fit()* method. \n","\n","This is the code we used in Week 2. \n","\n","```\n","from sklearn.linear_model import LinearRegression\n","lm = LinearRegression() # builds up model package \n","lm.fit(X_train, y_train) # trains model using training x and y\n","```\n","\n","Now we want to design a pipeline such that the entire *Tensorflow* objects can be thrown into the Sci-kit Learn framework. This is because Sci-kit Learn library provides nice cross-validation function and we want to take advantage of that function. \n","\n","To achieve this goal, we introduce a wrapper method. The syntax is *.wrappers* under Tensorflow Keras."],"metadata":{"id":"0He6RlRjp-5b"}},{"cell_type":"code","metadata":{"id":"WfD7EnWUwM94","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639328475583,"user_tz":480,"elapsed":283,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"d607a45d-5da4-4ee8-eb0b-a12d11c20d4e"},"source":["# create a KerasRegressor based on the model defined above\n","keras_reg = tf.keras.wrappers.scikit_learn.KerasRegressor(build_model)\n","\n","# comment:\n","# The KerasRegressor object is a think wrapper around the Keras model \n","# built using build_model(). Since we did not specify any hyperparameters \n","# when creating it, it will use the default hyperparameters we defined in \n","# build_model(). This makes things convenient because we can now use \n","# this object just like a regular Scikit-learn regressor. \n","# In other words, we can use .fit(), .predict(), and all these concepts\n","# consistently as we discussed before."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n","  \n"]}]},{"cell_type":"code","metadata":{"id":"upv7fnIbwcAV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639328485499,"user_tz":480,"elapsed":7619,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"31c3e0ae-4bf8-46c6-8cc4-e1cdf4543f75"},"source":["# fit the model\n","keras_reg.fit(X_train, y_train, epochs=10,\n","              validation_data=(X_valid, y_valid),\n","              callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","363/363 [==============================] - 1s 2ms/step - loss: 2.4956 - val_loss: 1.0730\n","Epoch 2/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.9038 - val_loss: 0.7929\n","Epoch 3/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.7408 - val_loss: 0.7334\n","Epoch 4/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.6925 - val_loss: 0.6996\n","Epoch 5/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.6620 - val_loss: 0.6724\n","Epoch 6/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.6369 - val_loss: 0.6492\n","Epoch 7/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.6165 - val_loss: 0.6287\n","Epoch 8/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.5983 - val_loss: 0.6088\n","Epoch 9/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.5819 - val_loss: 0.5932\n","Epoch 10/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.5678 - val_loss: 0.5778\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7ff13e460c10>"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"6XkVXKv1yWHQ"},"source":["# prediction on test set\n","y_test_pred = keras_reg.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HRr9NtxZyxxH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639328489658,"user_tz":480,"elapsed":18,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"cb157368-a8b8-4402-de4a-88967ced0759"},"source":["# mean square error on test set\n","import numpy as np\n","rmse_test = (np.sum((y_test_pred - y_test) ** 2) / len(y_test)) ** 0.5\n","rmse_test \n","\n","# Question: how to interpret this?\n","\n","# Answer: On the test set (observations the model has not seen before)\n","#         the educated guess from the model make predictions with error \n","#         range to be about "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7602871518974966"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"DhK45xbgxsEO"},"source":["Note that any extra parameter you pass to the *fit()* method will get passed to the underlying Keras model. Also note that the score will be the oppositie of the MSE because Scikit-Learn wants scores, not losses (i.e. higher should be better).\n","\n","We do not want to train and evaluate a single model like this, though we want to train hundreds of variants and see which one perfoms best on the validation set. Since there are many hyperparameters, it is preferable to use a randomized search."]},{"cell_type":"code","metadata":{"id":"8o15Wb6ByVLZ"},"source":["from scipy.stats import reciprocal\n","from sklearn.model_selection import RandomizedSearchCV"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awnpBGpxzuyA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639331710322,"user_tz":480,"elapsed":348,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"d4460a8b-f67d-4abd-d3a7-e38c11264725"},"source":["np.arange(0, 40, 10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0, 10, 20, 30])"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"sqrHXBbFzmLP"},"source":["param_distribs = {\n","    \"n_hidden\": [0, 1, 3],\n","    \"n_neurons\": np.arange(0, 40, 10)[1:],\n","    \"learning_rate\": reciprocal(3e-4, 3e-2)\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjMvcZGyz936"},"source":["# create randomized grid search\n","rnd_search_cv = RandomizedSearchCV(\n","    keras_reg, param_distributions=param_distribs,\n","    n_iter=10, cv=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bmRm91kn0XlO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639328721877,"user_tz":480,"elapsed":224174,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"eb375ce7-f3fe-4f60-ec47-505dd0fec76e"},"source":["# fit the above randomized grid search function\n","rnd_search_cv.fit(X_train, y_train, epochs=10,\n","                  validation_data=(X_valid, y_valid),\n","                  callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n","\n","# Go get a cup of a coffee! This may take a few minutes."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 3ms/step - loss: 1.2299 - val_loss: 3.5621\n","Epoch 2/10\n","242/242 [==============================] - 0s 2ms/step - loss: 10.4047 - val_loss: 122.8528\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 96.0350 - val_loss: 3745.5525\n","Epoch 4/10\n","242/242 [==============================] - 0s 2ms/step - loss: 5871.1348 - val_loss: 117391.8438\n","Epoch 5/10\n","242/242 [==============================] - 0s 2ms/step - loss: 602234.3125 - val_loss: 3581354.2500\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 4107613.0000 - val_loss: 111472584.0000\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 124005688.0000 - val_loss: 3483758336.0000\n","Epoch 8/10\n","242/242 [==============================] - 0s 2ms/step - loss: 8916257792.0000 - val_loss: 108371828736.0000\n","Epoch 9/10\n","242/242 [==============================] - 0s 2ms/step - loss: 54074744832.0000 - val_loss: 3153740169216.0000\n","Epoch 10/10\n","242/242 [==============================] - 0s 2ms/step - loss: 5437910941696.0000 - val_loss: 97235542999040.0000\n","121/121 [==============================] - 0s 1ms/step - loss: 20986007126016.0000\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["242/242 [==============================] - 1s 2ms/step - loss: 1.8702 - val_loss: 27.9017\n","Epoch 2/10\n","242/242 [==============================] - 0s 2ms/step - loss: 278.1375 - val_loss: 1825.5867\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 28783.2812 - val_loss: 119612.7656\n","Epoch 4/10\n","242/242 [==============================] - 0s 2ms/step - loss: 1178597.3750 - val_loss: 7998366.5000\n","Epoch 5/10\n","242/242 [==============================] - 0s 2ms/step - loss: 25339422.0000 - val_loss: 528441120.0000\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 555287424.0000 - val_loss: 33968760832.0000\n","Epoch 7/10\n","242/242 [==============================] - 0s 2ms/step - loss: 12792151040.0000 - val_loss: 2065856593920.0000\n","Epoch 8/10\n","242/242 [==============================] - 0s 2ms/step - loss: 26725530468352.0000 - val_loss: 135836200861696.0000\n","Epoch 9/10\n","242/242 [==============================] - 0s 2ms/step - loss: 793427629309952.0000 - val_loss: 9205057660649472.0000\n","Epoch 10/10\n","242/242 [==============================] - 0s 2ms/step - loss: 5539572582711296.0000 - val_loss: 576105197730791424.0000\n","121/121 [==============================] - 0s 1ms/step - loss: 17228750206795776.0000\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["242/242 [==============================] - 1s 3ms/step - loss: 4.1096 - val_loss: 0.5551\n","Epoch 2/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.7419 - val_loss: 0.5725\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5610 - val_loss: 1.3102\n","Epoch 4/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5542 - val_loss: 0.5891\n","Epoch 5/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5573 - val_loss: 0.5414\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 1.3064 - val_loss: 1.0266\n","Epoch 7/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6080 - val_loss: 0.5354\n","Epoch 8/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6970 - val_loss: 0.7310\n","Epoch 9/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6106 - val_loss: 0.5769\n","Epoch 10/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6617 - val_loss: 0.5344\n","121/121 [==============================] - 0s 1ms/step - loss: 0.5607\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["242/242 [==============================] - 1s 2ms/step - loss: 2.0659 - val_loss: 2.1579\n","Epoch 2/10\n","242/242 [==============================] - 0s 2ms/step - loss: 2.1273 - val_loss: 7.5324\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 4.9597 - val_loss: 33.3548\n","Epoch 4/10\n","242/242 [==============================] - 0s 2ms/step - loss: 24.4251 - val_loss: 139.4403\n","Epoch 5/10\n","242/242 [==============================] - 0s 2ms/step - loss: 73.7983 - val_loss: 578.5710\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 370.3467 - val_loss: 2439.9778\n","Epoch 7/10\n","242/242 [==============================] - 0s 2ms/step - loss: 2250.3020 - val_loss: 10254.6836\n","Epoch 8/10\n","242/242 [==============================] - 1s 2ms/step - loss: 8459.5820 - val_loss: 43096.0430\n","Epoch 9/10\n","242/242 [==============================] - 0s 2ms/step - loss: 22070.4355 - val_loss: 183203.4531\n","Epoch 10/10\n","242/242 [==============================] - 0s 2ms/step - loss: 163238.2031 - val_loss: 766077.0625\n","121/121 [==============================] - 0s 1ms/step - loss: 164903.9219\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["242/242 [==============================] - 1s 2ms/step - loss: 1.7175 - val_loss: 0.7542\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.7901 - val_loss: 0.6989\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6323 - val_loss: 0.6563\n","Epoch 4/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6069 - val_loss: 0.6052\n","Epoch 5/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5794 - val_loss: 0.5631\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5606 - val_loss: 0.5578\n","Epoch 7/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5656 - val_loss: 0.5464\n","Epoch 8/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5478 - val_loss: 0.5399\n","Epoch 9/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5477 - val_loss: 0.5421\n","Epoch 10/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5446 - val_loss: 0.5632\n","121/121 [==============================] - 0s 1ms/step - loss: 0.5601\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 2ms/step - loss: 2.0612 - val_loss: 0.6865\n","Epoch 2/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5635 - val_loss: 0.5357\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5382 - val_loss: 0.5352\n","Epoch 4/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5361 - val_loss: 0.5366\n","Epoch 5/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5373 - val_loss: 0.5406\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5523 - val_loss: 0.5431\n","Epoch 7/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5278 - val_loss: 0.5594\n","Epoch 8/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5394 - val_loss: 0.5441\n","Epoch 9/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5305 - val_loss: 0.5396\n","Epoch 10/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5406 - val_loss: 0.5420\n","121/121 [==============================] - 0s 1ms/step - loss: 0.5628\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 2ms/step - loss: 3.1310 - val_loss: 1.2839\n","Epoch 2/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.9883 - val_loss: 0.7815\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.7474 - val_loss: 0.7186\n","Epoch 4/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6991 - val_loss: 0.6849\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6691 - val_loss: 0.6623\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6461 - val_loss: 0.6457\n","Epoch 7/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6315 - val_loss: 0.6250\n","Epoch 8/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6137 - val_loss: 0.6126\n","Epoch 9/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6017 - val_loss: 0.6002\n","Epoch 10/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5911 - val_loss: 0.5887\n","121/121 [==============================] - 0s 1ms/step - loss: 0.5949\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["242/242 [==============================] - 1s 2ms/step - loss: 3.3246 - val_loss: 1.2827\n","Epoch 2/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.9616 - val_loss: 0.7210\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6947 - val_loss: 0.6662\n","Epoch 4/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6554 - val_loss: 0.6369\n","Epoch 5/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6314 - val_loss: 0.6181\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6168 - val_loss: 0.6046\n","Epoch 7/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6022 - val_loss: 0.5910\n","Epoch 8/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5914 - val_loss: 0.5822\n","Epoch 9/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5825 - val_loss: 0.5718\n","Epoch 10/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5721 - val_loss: 0.5675\n","121/121 [==============================] - 0s 1ms/step - loss: 0.5661\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 2ms/step - loss: 3.1418 - val_loss: 1.2867\n","Epoch 2/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.8086 - val_loss: 0.7666\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6083 - val_loss: 0.6902\n","Epoch 4/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5806 - val_loss: 0.6591\n","Epoch 5/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5675 - val_loss: 0.6404\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5620 - val_loss: 0.6265\n","Epoch 7/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5546 - val_loss: 0.6191\n","Epoch 8/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5527 - val_loss: 0.6099\n","Epoch 9/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5478 - val_loss: 0.6023\n","Epoch 10/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5426 - val_loss: 0.5928\n","121/121 [==============================] - 0s 1ms/step - loss: 0.6157\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["242/242 [==============================] - 1s 2ms/step - loss: 0.8996 - val_loss: 0.6533\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6326 - val_loss: 0.9220\n","Epoch 3/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.7429 - val_loss: 0.4891\n","Epoch 4/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4935 - val_loss: 0.4649\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4478 - val_loss: 0.4412\n","Epoch 6/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4258 - val_loss: 0.4275\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4180 - val_loss: 0.4168\n","Epoch 8/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4119 - val_loss: 0.4165\n","Epoch 9/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4124 - val_loss: 0.4098\n","Epoch 10/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4093 - val_loss: 0.4226\n","121/121 [==============================] - 0s 1ms/step - loss: 0.4118\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 3ms/step - loss: 1.1074 - val_loss: 0.6103\n","Epoch 2/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5491 - val_loss: 0.5075\n","Epoch 3/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4832 - val_loss: 0.4685\n","Epoch 4/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4545 - val_loss: 0.4464\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4389 - val_loss: 0.4391\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.4327 - val_loss: 0.4305\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4267 - val_loss: 0.4235\n","Epoch 8/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4278 - val_loss: 0.4270\n","Epoch 9/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4206 - val_loss: 0.4186\n","Epoch 10/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4161 - val_loss: 0.4151\n","121/121 [==============================] - 0s 1ms/step - loss: 0.4097\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["242/242 [==============================] - 1s 2ms/step - loss: 1.0912 - val_loss: 0.9309\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6246 - val_loss: 0.7037\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5020 - val_loss: 0.4975\n","Epoch 4/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.4528 - val_loss: 0.4590\n","Epoch 5/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.4384 - val_loss: 0.4517\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.4475 - val_loss: 0.6397\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4772 - val_loss: 0.4530\n","Epoch 8/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4286 - val_loss: 0.4295\n","Epoch 9/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4120 - val_loss: 0.4228\n","Epoch 10/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.4026 - val_loss: 0.4119\n","121/121 [==============================] - 0s 1ms/step - loss: 0.4364\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 2ms/step - loss: 1.0783 - val_loss: 0.7313\n","Epoch 2/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.7457 - val_loss: 22.2423\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 30.2497 - val_loss: 557.6595\n","Epoch 4/10\n","242/242 [==============================] - 0s 2ms/step - loss: 1535.2266 - val_loss: 15487.5684\n","Epoch 5/10\n","242/242 [==============================] - 0s 2ms/step - loss: 4196.1646 - val_loss: 428264.3750\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 916961.3750 - val_loss: 11705624.0000\n","Epoch 7/10\n","242/242 [==============================] - 0s 2ms/step - loss: 3275587.7500 - val_loss: 327791328.0000\n","Epoch 8/10\n","242/242 [==============================] - 0s 2ms/step - loss: 771717440.0000 - val_loss: 8978358272.0000\n","Epoch 9/10\n","242/242 [==============================] - 0s 2ms/step - loss: 8059041280.0000 - val_loss: 249364103168.0000\n","Epoch 10/10\n","242/242 [==============================] - 0s 2ms/step - loss: 303428173824.0000 - val_loss: 6796638945280.0000\n","121/121 [==============================] - 0s 1ms/step - loss: 1460124909568.0000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 2ms/step - loss: 1.0102 - val_loss: 6.0897\n","Epoch 2/10\n","242/242 [==============================] - 0s 2ms/step - loss: 1.5922 - val_loss: 48.4175\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 520.9161 - val_loss: 825.4350\n","Epoch 4/10\n","242/242 [==============================] - 0s 2ms/step - loss: 151.9227 - val_loss: 13309.2129\n","Epoch 5/10\n","242/242 [==============================] - 0s 2ms/step - loss: 5869.3237 - val_loss: 219484.0312\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 1737626.8750 - val_loss: 3505030.0000\n","Epoch 7/10\n","242/242 [==============================] - 0s 2ms/step - loss: 25001408.0000 - val_loss: 58466004.0000\n","Epoch 8/10\n","242/242 [==============================] - 0s 2ms/step - loss: 326239360.0000 - val_loss: 989168896.0000\n","Epoch 9/10\n","242/242 [==============================] - 0s 2ms/step - loss: 1114880896.0000 - val_loss: 16282927104.0000\n","Epoch 10/10\n","242/242 [==============================] - 0s 2ms/step - loss: 14474456064.0000 - val_loss: 280266014720.0000\n","121/121 [==============================] - 0s 1ms/step - loss: 8360816640.0000\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["242/242 [==============================] - 1s 2ms/step - loss: 0.9020 - val_loss: 0.6360\n","Epoch 2/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6525 - val_loss: 0.6541\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6526 - val_loss: 0.5332\n","Epoch 4/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5539 - val_loss: 0.6913\n","Epoch 5/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.9716 - val_loss: 0.5684\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.8999 - val_loss: 0.5561\n","Epoch 7/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6061 - val_loss: 0.5203\n","Epoch 8/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5612 - val_loss: 0.6125\n","Epoch 9/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5457 - val_loss: 0.6229\n","Epoch 10/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.6324 - val_loss: 0.6970\n","121/121 [==============================] - 0s 1ms/step - loss: 0.7879\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 2ms/step - loss: 1.3680 - val_loss: 1.0469\n","Epoch 2/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.9771 - val_loss: 4.1577\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 3.3810 - val_loss: 0.6930\n","Epoch 4/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5798 - val_loss: 0.5271\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4984 - val_loss: 0.4821\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.4725 - val_loss: 0.4685\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4555 - val_loss: 0.4482\n","Epoch 8/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4431 - val_loss: 0.4360\n","Epoch 9/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4373 - val_loss: 0.4307\n","Epoch 10/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.4301 - val_loss: 0.4249\n","121/121 [==============================] - 0s 1ms/step - loss: 0.4107\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 3ms/step - loss: 1.5167 - val_loss: 0.8442\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.7200 - val_loss: 0.6509\n","Epoch 3/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6174 - val_loss: 0.5943\n","Epoch 4/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5635 - val_loss: 0.5417\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5264 - val_loss: 0.5143\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.4974 - val_loss: 0.4855\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4761 - val_loss: 0.4717\n","Epoch 8/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.4624 - val_loss: 0.4572\n","Epoch 9/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4514 - val_loss: 0.4478\n","Epoch 10/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4442 - val_loss: 0.4439\n","121/121 [==============================] - 0s 1ms/step - loss: 0.4365\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 3ms/step - loss: 1.0925 - val_loss: 0.6402\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5884 - val_loss: 0.5836\n","Epoch 3/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5395 - val_loss: 0.5325\n","Epoch 4/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5046 - val_loss: 0.4978\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4813 - val_loss: 0.4884\n","Epoch 6/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4654 - val_loss: 0.4691\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4513 - val_loss: 0.4538\n","Epoch 8/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4415 - val_loss: 0.4458\n","Epoch 9/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4318 - val_loss: 0.4399\n","Epoch 10/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4248 - val_loss: 0.4379\n","121/121 [==============================] - 0s 2ms/step - loss: 0.4526\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["242/242 [==============================] - 1s 3ms/step - loss: 1.5320 - val_loss: 1.2038\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.9468 - val_loss: 0.9188\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.7485 - val_loss: 0.5723\n","Epoch 4/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5497 - val_loss: 0.5363\n","Epoch 5/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5193 - val_loss: 0.5121\n","Epoch 6/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.4995 - val_loss: 0.4918\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4845 - val_loss: 0.4790\n","Epoch 8/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.4726 - val_loss: 0.4714\n","Epoch 9/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.4636 - val_loss: 0.4633\n","Epoch 10/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4547 - val_loss: 0.4527\n","121/121 [==============================] - 0s 1ms/step - loss: 0.4450\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 3ms/step - loss: 1.3086 - val_loss: 0.8429\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6824 - val_loss: 0.6210\n","Epoch 3/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5735 - val_loss: 0.5609\n","Epoch 4/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5301 - val_loss: 0.5266\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5048 - val_loss: 0.5052\n","Epoch 6/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4888 - val_loss: 0.4913\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4782 - val_loss: 0.4772\n","Epoch 8/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4687 - val_loss: 0.4677\n","Epoch 9/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4613 - val_loss: 0.4602\n","Epoch 10/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4560 - val_loss: 0.4541\n","121/121 [==============================] - 0s 1ms/step - loss: 0.4573\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 2ms/step - loss: 1.2706 - val_loss: 0.7551\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6748 - val_loss: 0.6620\n","Epoch 3/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6017 - val_loss: 0.6101\n","Epoch 4/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5603 - val_loss: 0.5717\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5299 - val_loss: 0.5440\n","Epoch 6/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5070 - val_loss: 0.5210\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4904 - val_loss: 0.5006\n","Epoch 8/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4740 - val_loss: 0.4822\n","Epoch 9/10\n","242/242 [==============================] - 1s 3ms/step - loss: 0.4629 - val_loss: 0.4726\n","Epoch 10/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4508 - val_loss: 0.4623\n","121/121 [==============================] - 0s 1ms/step - loss: 0.4725\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["242/242 [==============================] - 1s 3ms/step - loss: 1.7770 - val_loss: 0.9756\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.8746 - val_loss: 0.8420\n","Epoch 3/10\n","242/242 [==============================] - 1s 3ms/step - loss: 0.7748 - val_loss: 0.7733\n","Epoch 4/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.7242 - val_loss: 0.7322\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6915 - val_loss: 0.7090\n","Epoch 6/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6684 - val_loss: 0.6844\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6499 - val_loss: 0.6657\n","Epoch 8/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6349 - val_loss: 0.6504\n","Epoch 9/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6215 - val_loss: 0.6363\n","Epoch 10/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6101 - val_loss: 0.6247\n","121/121 [==============================] - 0s 1ms/step - loss: 0.6190\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 3ms/step - loss: 3.3834 - val_loss: 1.4309\n","Epoch 2/10\n","242/242 [==============================] - 1s 3ms/step - loss: 1.1787 - val_loss: 1.0045\n","Epoch 3/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.9275 - val_loss: 0.8546\n","Epoch 4/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.8142 - val_loss: 0.7765\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.7569 - val_loss: 0.7309\n","Epoch 6/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.7217 - val_loss: 0.7087\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.7041 - val_loss: 0.6910\n","Epoch 8/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6897 - val_loss: 0.6786\n","Epoch 9/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6766 - val_loss: 0.6687\n","Epoch 10/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6689 - val_loss: 0.6587\n","121/121 [==============================] - 0s 1ms/step - loss: 0.6499\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 3ms/step - loss: 2.7111 - val_loss: 1.4395\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 1.2264 - val_loss: 1.0490\n","Epoch 3/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.9360 - val_loss: 0.8673\n","Epoch 4/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.8023 - val_loss: 0.7762\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.7243 - val_loss: 0.7187\n","Epoch 6/10\n","242/242 [==============================] - 1s 3ms/step - loss: 0.6745 - val_loss: 0.6747\n","Epoch 7/10\n","242/242 [==============================] - 1s 3ms/step - loss: 0.6404 - val_loss: 0.6465\n","Epoch 8/10\n","242/242 [==============================] - 1s 3ms/step - loss: 0.6161 - val_loss: 0.6258\n","Epoch 9/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5979 - val_loss: 0.6062\n","Epoch 10/10\n","242/242 [==============================] - 1s 3ms/step - loss: 0.5833 - val_loss: 0.5920\n","121/121 [==============================] - 0s 1ms/step - loss: 0.6058\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 3ms/step - loss: 2.9038 - val_loss: 1.5522\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 1.3495 - val_loss: 1.1198\n","Epoch 3/10\n","242/242 [==============================] - 1s 2ms/step - loss: 1.0681 - val_loss: 0.9516\n","Epoch 4/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.9165 - val_loss: 0.8492\n","Epoch 5/10\n","242/242 [==============================] - 1s 3ms/step - loss: 0.8235 - val_loss: 0.7802\n","Epoch 6/10\n","242/242 [==============================] - 1s 3ms/step - loss: 0.7621 - val_loss: 0.7364\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.7181 - val_loss: 0.7018\n","Epoch 8/10\n","242/242 [==============================] - 1s 3ms/step - loss: 0.6872 - val_loss: 0.6781\n","Epoch 9/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6666 - val_loss: 0.6617\n","Epoch 10/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6500 - val_loss: 0.6445\n","121/121 [==============================] - 0s 1ms/step - loss: 0.6427\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["242/242 [==============================] - 1s 3ms/step - loss: 2.9397 - val_loss: 1.2545\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 1.1397 - val_loss: 1.0877\n","Epoch 3/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.8957 - val_loss: 0.7801\n","Epoch 4/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.7551 - val_loss: 0.7117\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6948 - val_loss: 0.6693\n","Epoch 6/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6537 - val_loss: 0.6407\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6250 - val_loss: 0.6133\n","Epoch 8/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6016 - val_loss: 0.5914\n","Epoch 9/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5824 - val_loss: 0.5742\n","Epoch 10/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5658 - val_loss: 0.5608\n","121/121 [==============================] - 0s 1ms/step - loss: 0.5560\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 3ms/step - loss: 2.2008 - val_loss: 1.1773\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.9422 - val_loss: 0.8600\n","Epoch 3/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.7658 - val_loss: 0.7714\n","Epoch 4/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.7116 - val_loss: 0.7360\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6839 - val_loss: 0.7126\n","Epoch 6/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6635 - val_loss: 0.6904\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6446 - val_loss: 0.6707\n","Epoch 8/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6283 - val_loss: 0.6540\n","Epoch 9/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6132 - val_loss: 0.6392\n","Epoch 10/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5984 - val_loss: 0.6211\n","121/121 [==============================] - 0s 1ms/step - loss: 0.6121\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 2ms/step - loss: 1.5995 - val_loss: 0.6676\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5679 - val_loss: 0.5352\n","Epoch 3/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.5065 - val_loss: 0.4931\n","Epoch 4/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4766 - val_loss: 0.4692\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4561 - val_loss: 0.4521\n","Epoch 6/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4392 - val_loss: 0.4376\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4288 - val_loss: 0.4248\n","Epoch 8/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4214 - val_loss: 0.4205\n","Epoch 9/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4162 - val_loss: 0.4215\n","Epoch 10/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4133 - val_loss: 0.4166\n","121/121 [==============================] - 0s 1ms/step - loss: 0.4166\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 3ms/step - loss: 0.9287 - val_loss: 0.6408\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5426 - val_loss: 0.4999\n","Epoch 3/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4810 - val_loss: 0.4742\n","Epoch 4/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4628 - val_loss: 0.4587\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4523 - val_loss: 0.4513\n","Epoch 6/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4442 - val_loss: 0.4444\n","Epoch 7/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4379 - val_loss: 0.4477\n","Epoch 8/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4331 - val_loss: 0.4332\n","Epoch 9/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4284 - val_loss: 0.4350\n","Epoch 10/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4254 - val_loss: 0.4319\n","121/121 [==============================] - 0s 1ms/step - loss: 0.4126\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(SGD, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","242/242 [==============================] - 1s 3ms/step - loss: 1.0529 - val_loss: 0.7808\n","Epoch 2/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.6617 - val_loss: 0.5978\n","Epoch 3/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5552 - val_loss: 0.5434\n","Epoch 4/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.5148 - val_loss: 0.5130\n","Epoch 5/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4936 - val_loss: 0.4792\n","Epoch 6/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4535 - val_loss: 0.4502\n","Epoch 7/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.4414 - val_loss: 0.4430\n","Epoch 8/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4436 - val_loss: 0.4397\n","Epoch 9/10\n","242/242 [==============================] - 0s 2ms/step - loss: 0.4241 - val_loss: 0.4350\n","Epoch 10/10\n","242/242 [==============================] - 1s 2ms/step - loss: 0.4180 - val_loss: 0.4311\n","121/121 [==============================] - 0s 1ms/step - loss: 0.4407\n","Epoch 1/10\n","363/363 [==============================] - 1s 2ms/step - loss: 1.1190 - val_loss: 0.6440\n","Epoch 2/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.6638 - val_loss: 0.5998\n","Epoch 3/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.5520 - val_loss: 0.5013\n","Epoch 4/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.4827 - val_loss: 0.4733\n","Epoch 5/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.4588 - val_loss: 0.4491\n","Epoch 6/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.4364 - val_loss: 0.4417\n","Epoch 7/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.4289 - val_loss: 0.4506\n","Epoch 8/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.4217 - val_loss: 0.4187\n","Epoch 9/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.4117 - val_loss: 0.4164\n","Epoch 10/10\n","363/363 [==============================] - 1s 2ms/step - loss: 0.4093 - val_loss: 0.4132\n"]},{"output_type":"execute_result","data":{"text/plain":["RandomizedSearchCV(cv=3,\n","                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x7ff13dfb9f50>,\n","                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7ff142426110>,\n","                                        'n_hidden': [0, 1, 3],\n","                                        'n_neurons': array([10, 20, 30])})"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"iK3RzkMV0tWw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639329193860,"user_tz":480,"elapsed":307,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"bc1dedea-790b-4a46-a97d-8ca535dbb7be"},"source":["# after a long wait, we output the best parameters\n","rnd_search_cv.best_params_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'learning_rate': 0.014032007606067277, 'n_hidden': 1, 'n_neurons': 10}"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"qmyDnWlE08Ci","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639329195522,"user_tz":480,"elapsed":3,"user":{"displayName":"Noble Kennamer","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05456406452697780453"}},"outputId":"e2f1cf3d-28f6-416e-9bae-54ff05caf143"},"source":["# print the score too\n","rnd_search_cv.best_score_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.4192752738793691"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"NMKfTOHI3bpy"},"source":["# extract the best model\n","best_model = rnd_search_cv.best_estimator_.model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dF_3Z2r73n54"},"source":["# save the model (this is optional)\n","# model.save(\"best_model_I_just_trained.h5\")\n","\n","# Remark:\n","# This is optional, but we write this remark so you are aware\n","# of this procedure. In practice, the models are big and complex\n","# enough that sometimes there are multiple teams develop the same\n","# model. In this case, it is often times a safe practice to save \n","# your model by using model.save(\"GIVE_IT_A_NAME.h5\"). The format\n","# must be h5 format, so please only change the name of the file.\n","# In Colab, after you run the model.save() code successfully, you \n","# will be able to see this by navigating to \"Content\" using the left \n","# menu bar. Go to the \"folder\" button and choose \"Content\"."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Investigation ends here."],"metadata":{"id":"X7codj6qnbcY"}}]}